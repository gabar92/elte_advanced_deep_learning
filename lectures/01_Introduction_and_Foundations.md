# Lecture 1: Introduction and Foundations

## What is this lecture about?

This lecture tries to answer the following questions:

1. What is the current state of NLP?
   1. Motivation with the latest NLP-based applications?
2. What do we understand under Natural Language Processing (NLP)?
3. What is the reason behind the explosion-like adaptation of NLP-based applications?
4. What are the prevalent tasks and belonging applications in NLP?
5. What are the NLP-specific challenges and what are the common solutions?
6. What are the advantages of the field NLP?
7. How NLP affected Computer Vision (CV) and vica versa?
8. What is the core concept of (almost) every Language Model?
9. What is the representation of textual data Neural Networks can ingest?
   1. What are Character Encoding Standards?
      1. What is the dynamics of the evolution of Character Encoding Standards?
   2. What is tokenization?
      1. What methods are used today?
   3. What are embeddings?
      1. What are the requirements of embeddings?
      2. What is the dynamics of the evolution of Embedding learning?
      3. How text embeddings opened the door for great applications?

## Contents:
* Motivation and Demo
* Definition of NLP
* Why is NLP so successful
* NLP-specific challenges
* NLP-specific advantages
* Converging paths: adopting techniques between NLP and Computer Vision (CV)
* The soul / gist of NLP models: Next word prediction
* A list of Tasks and Application in NLP
* Classical methods (outdated methods)
* Character Encoding Standards
* Tokenization and Embeddings 
  * Tokenization
  * Embeddings
  * Text embeddings