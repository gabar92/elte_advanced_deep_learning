# Lecture 3: Large Language Models

## What is this lecture about?

This lecture tries to answer the following questions:

## <ins>Lecture 3: Large Language Models</ins>

### What is this lecture about?

This lecture tries to answer the following questions:
1. What are the stages of LLM training?
2. What are the new techniques behind the success of ChatGPT?
3. I want to deploy a model on my custom problem / data. How can I do it?
4. How can I adopt the different LLM models in my own application?

### Content of the lecture:

Emerging properties of LLMs:
* In-context Learning
* Zero-shot / Few-shot learning
* Chain-of-Thought prompting

Scaling laws:
* model size: parameters
* dataset size

(we do not really use the other objectives than MLM)
Diagrams:
* diagram of data sizes for training
* diagram of model sizes

Generalization?

Pre-trained vs. Fine-tuned models

GPT series:
* GPT-1, GPT-2, GPT-3, GPT-3.5, GPT-4
* ChatGPT
* InstructGPT
* ChatGPT, BARD

RLHF: Reinforcement Learning Human Feedback

Proprietary vs. Open-source models
* Proprietary: API
* Open-source: paper, architecture, weights

List of LLMs with reference