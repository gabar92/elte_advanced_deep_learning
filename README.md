# Advanced Deep Learning course on ELTE (2024)
Lecture contents for Advanced Deep Learning lecture in ELTE

---

## <ins>Lecture 1: Introduction and Foundations</ins>

### What is this lecture about?

This lecture tries to answer the following questions:

1. What is the current state of Natural Language Processing (NLP) across various applications?
2. How do we define Natural Language Processing (NLP)?
3. What has led to the rapid adoption of NLP-based applications?
4. What are the key tasks and associated applications in NLP?
5. What are the challenges and advantages of NLP, along with common solutions?
7. How Computer Vision (CV) and NLP inspired each other's best practices?
8. What is the foundational principle underlying nearly every Language Model?
9. How textual data is represented for Language Model processing?

### Content of the lecture:
* **Motivation**:
  * Demo of recent NLP-related applications
    * showcasing a couple of recent applications, softwares, and devices which use the NLP-related state-of-the-art AI advancements.
  * Definition of Natural Language Processing
    * giving an informal definition of NLP. Setting NLP among related disciplines and domains.
  * Reasons behind the quick adaption of NLP-based applications
    * focusing on the potential traits and reasons what makes recent NLP-related AI applications such successful and broadly adopted.

* **Bird's Eye View**:
  * NLP-specific challenges
    * providing a list of challenges being present or being special to the NLP field
      * discrete data; lack of standard representation; lack of inherent structure; sparsity of data;
        variable length of input; handling long-range inputs and capturing long-range dependencies;
        labeling for some tasks is very challenging; evaluation of downstream tasks
  * NLP-specific advantages
    * providing a list of advantages and beneficial features of the NLP field
      * abundance of data; unsupervised and self-supervised learning provide strong general purpose models;
        transfer learning efficacy; emergent properties;
  * Converging paths: adopting techniques between NLP and CV
    * presenting a couple of best-practice techniques adopted between CV and NLP
      * CV --> NLP: two-tage training procedure: pre-training then fine-tuning;
      * NLP --> CV: unsupervised / self-supervised learning; transformer architecture; embeddings
  * The gist of NLP models: Next Word Prediction
    * introducing the Language Modeling (Next Word Prediction) concept behind most of the models. [TODO]
  * A list of Tasks and Applications in NLP
    * providing a list of the better-known NLP tasks with a couple of applications
  
* **Details**:
  * Classical methods:
    * 
  * Character Encodings 
  * Tokenization and Embeddings 
  * Tokenization 
  * Embeddings 
  * Text Embeddings

* Additional Resources
  * providing a couple of interesting sources

---

## <ins>Lecture 2: Language Modeling</ins>

---

## <ins>Lecture 3: Large Language Models</ins>

---

## <ins>Lecture 4: Research</ins>

---

